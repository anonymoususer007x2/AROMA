{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/anonymoususer007x2/AROMA"
      ],
      "metadata": {
        "id": "RSkA4OIhVD80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13aUpYdjgpfA"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install anthropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IC96dD-CgsZa"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import anthropic\n",
        "import pandas as pd\n",
        "import re\n",
        "import requests\n",
        "import numpy as np\n",
        "import os\n",
        "from threading import Thread\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.spatial.distance import cosine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swTMWAgMerwx"
      },
      "outputs": [],
      "source": [
        "# file_path = '/content/AROMA/humanfeedback_category_proposer.csv'\n",
        "# file_path='/content/AROMA/codex_ds1000_results'\n",
        "# file_path='/content/AROMA/Claude_12_gsm8k_NO_cot.csv'\n",
        "data=pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncomment to retrieve data for the DS-1000 Pandas or Numpy results"
      ],
      "metadata": {
        "id": "Rv6ksbwBMwBg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDT6Ikw8UL7g"
      },
      "outputs": [],
      "source": [
        "# data = data[data['library'] == 'Pandas'].copy()\n",
        "# data = data[data['library'] == 'Numpy'].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4wxWcofukN3"
      },
      "outputs": [],
      "source": [
        "incorrect_samples=data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insert your key here"
      ],
      "metadata": {
        "id": "S8ErEiQlWZ8P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQuCtVjnln30"
      },
      "outputs": [],
      "source": [
        "openai.api_key = ''\n",
        "client = anthropic.Anthropic(\n",
        "    api_key=\"\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter to get only rows where 'correct' column is False. For the human feedback data uncomment the line with 'Llama_lost' to get samples LLaMA-3 70B was preferred or not to other models. You can also generate categories with all rows.\n"
      ],
      "metadata": {
        "id": "QQKn57PPM6SX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4wQMF4IDjEB"
      },
      "outputs": [],
      "source": [
        "incorrect_samples = incorrect_samples[incorrect_samples['is_correct'] == False]\n",
        "# incorrect_samples = incorrect_samples[incorrect_samples['Llama_lost'] == True]\n",
        "random_seed = 20\n",
        "shuffled_incorrect_samples = incorrect_samples.sample(frac=1, random_state=random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Category Proposer**"
      ],
      "metadata": {
        "id": "PqPdhidNUid4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yDFtFu4fYiE"
      },
      "outputs": [],
      "source": [
        "def get_general_failures(incorrect_samples, chunk_size=60, num_chunks=None, process_all=False):\n",
        "    total_items = len(incorrect_samples)\n",
        "    all_responses = {}\n",
        "\n",
        "    if process_all:\n",
        "        num_chunks = total_items // chunk_size + (1 if total_items % chunk_size else 0)\n",
        "\n",
        "    end_limit = min(num_chunks * chunk_size, total_items) if num_chunks else total_items\n",
        "    max_attempts = 5\n",
        "\n",
        "    for start_index in range(0, end_limit, chunk_size):\n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                end_index = min(start_index + chunk_size, total_items)\n",
        "                question_samples = incorrect_samples['question'][start_index:end_index].tolist()\n",
        "\n",
        "                formatted_samples = '\\n'.join([f\"- {question}\" for question in question_samples])\n",
        "\n",
        "                prompt_text = \"\"\"\n",
        "                I will provide a series of data for you to remember. Subsequently, I will ask you some\n",
        "                questions to test your performance! Here are questions for you to memorize.\n",
        "\n",
        "                {}\n",
        "\n",
        "                The above questions were incorrectly answered by the machine learning model.\n",
        "                Using these specific examples, are there any tasks that are shared among\n",
        "                the questions the model answers incorrectly?\n",
        "\n",
        "                For each identified task, please offer a detailed and precise description.\n",
        "                Highlight the particular characteristics of the tasks that the model failed to answer correctly.\n",
        "                Do not describe why the model failed but rather the patterns within the questions.\n",
        "\n",
        "                Please focus on achieving a high level of specificity and clarity in your categorization,\n",
        "                steering clear of broad or vague descriptions such as:\n",
        "                'Subject-Specific Knowledge Requirement', 'Task Ambiguity',\n",
        "                'Context-Dependent Jargon Interpretation', 'Specialized Domain Knowledge Required', 'Complex Question Mishandling',\n",
        "                'Ambiguously Phrased Question', 'Implicit Contextual Understanding'\n",
        "                'Implicit Language Competency Assumption', 'Specific Language Processing Tasks', or\n",
        "                'Inference-Based Questions.'\n",
        "\n",
        "\n",
        "                We will evaluate the effectiveness and generalizability of the patterns descriptions using a separate set of unseen questions.\n",
        "                Each description should be clear, precise, and comprehensive enough to allow us to reliably identify and categorize new questions\n",
        "                that exhibit the same pattern characteristics, even if they differ in their specific content or phrasing.\n",
        "\n",
        "                For each failure, please focus on isolating a single task among the questions,\n",
        "                rather than grouping multiple tasks under a single failure mode.\n",
        "\n",
        "                Instead of discussing the content of the questions within each failure mode description,\n",
        "                please provide two short snippets of questions that demonstrate the identified task.\n",
        "\n",
        "                The desired format for presenting each failure mode analysis is as follows:\n",
        "\n",
        "                1. **Task Title**: Provide a detailed description of the specific pattern observed in the questions that were answered incorrectly.\n",
        "\n",
        "                - Example Question 1 (Demonstrating Failure Mode 1)\n",
        "                - Example Question 2 (Demonstrating Failure Mode 1)\n",
        "\n",
        "                2. **Task Title**: Provide a detailed description of the specific pattern observed in the questions that were answered incorrectly.\n",
        "\n",
        "                - Example Question 1 (Demonstrating Failure Mode 2)\n",
        "                - Example Question 2 (Demonstrating Failure Mode 2)\n",
        "\n",
        "\n",
        "                \"\"\".format(formatted_samples)\n",
        "\n",
        "                response = openai.chat.completions.create(\n",
        "                    model=\"gpt-4-1106-preview\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"user\", \"content\": prompt_text}\n",
        "                    ],\n",
        "                    temperature=1,\n",
        "                    max_tokens=4000,\n",
        "                    top_p=1,\n",
        "                    frequency_penalty=0,\n",
        "                    presence_penalty=0\n",
        "                )\n",
        "                # response = client.messages.create(\n",
        "                #     model=\"claude-3-opus-20240229\",\n",
        "                #     messages=[\n",
        "                #         {\"role\": \"user\", \"content\": prompt_text}\n",
        "                #     ],\n",
        "                #     temperature=1,\n",
        "                #     max_tokens=3400,\n",
        "                #     top_p=1,\n",
        "                # )\n",
        "\n",
        "\n",
        "                #openAI\n",
        "                all_responses[f'chunk_{start_index}_{end_index}'] = response.choices[0].message.content\n",
        "\n",
        "                #claude\n",
        "                # all_responses[f'chunk_{start_index}_{end_index}'] = response.content[0].text.strip()\n",
        "\n",
        "                break\n",
        "\n",
        "            except openai.BadRequestError as e:\n",
        "                if 'maximum context length' in str(e):\n",
        "                    chunk_size = chunk_size - 5\n",
        "                    if chunk_size <= 0:\n",
        "                        raise ValueError(\"Chunk size reduced to zero, cannot proceed with requests.\")\n",
        "                else:\n",
        "                    raise\n",
        "\n",
        "            if (start_index // chunk_size + 1) >= num_chunks:\n",
        "                break\n",
        "\n",
        "    return all_responses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tYp6S-hWg1h"
      },
      "outputs": [],
      "source": [
        "response_dict = get_general_failures(shuffled_incorrect_samples, num_chunks=1)\n",
        "for chunk_key, response_content in response_dict.items():\n",
        "    print(f\"Chunk: {chunk_key}\")\n",
        "    print(\"Response Content:\")\n",
        "    print(response_content)\n",
        "    print(\"\\n-----------------------\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IryCadQW4pQe"
      },
      "outputs": [],
      "source": [
        "failures = pd.DataFrame(columns=[\"Failure Mode Title\", \"Failure Mode Description\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuX9KRdm1pod"
      },
      "outputs": [],
      "source": [
        "def process_chunk(chunk):\n",
        "    data = []\n",
        "    pattern = r\"\\d+\\.\\s\\*\\*(.*?)\\*\\*:\\s(.*?)(?=\\n\\s+-|\\Z)\"\n",
        "\n",
        "    matches = re.findall(pattern, chunk, re.DOTALL)\n",
        "\n",
        "    for title, description in matches:\n",
        "        title = title.strip()\n",
        "        description = description.strip()\n",
        "        description = re.sub(r\"\\n\\s+-.*\", \"\", description, flags=re.DOTALL).strip()\n",
        "        data.append({\"Failure Mode Title\": title, \"Failure Mode Description\": description})\n",
        "\n",
        "    df_chunk = pd.DataFrame(data)\n",
        "    return df_chunk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QM4oy6hh7X2q"
      },
      "outputs": [],
      "source": [
        "df_list = [process_chunk(chunk) for chunk_key, chunk in response_dict.items()]\n",
        "\n",
        "failures_df = pd.concat(df_list, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYL4h8ctMtKi"
      },
      "outputs": [],
      "source": [
        "failures_df['Failure Mode Title'] = failures_df['Failure Mode Title'].str.replace('Failure Mode:', '', regex=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Falsifier**"
      ],
      "metadata": {
        "id": "nz32duFWUdXY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zan9_NDP_-yq"
      },
      "outputs": [],
      "source": [
        "class ClaudeQueryProcessor:\n",
        "    def __init__(self, client):\n",
        "        \"\"\"\n",
        "        Initialize the ClaudeQueryProcessor instance with a Claude client.\n",
        "\n",
        "        Parameters:\n",
        "        - client: Initialized Claude Chat API client.\n",
        "        \"\"\"\n",
        "        self.client = client\n",
        "\n",
        "    def _interpret_response(self, response):\n",
        "        \"\"\"\n",
        "        Interpret the Claude response to determine the alignment status of the question.\n",
        "        \"\"\"\n",
        "        if 'Conclusion: Aligned' in response:\n",
        "            return 'Yes'\n",
        "        elif 'Conclusion: Unaligned' in response:\n",
        "            return 'No'\n",
        "        elif 'Conclusion: Uncertain' in response:\n",
        "            return 'Uncertain'\n",
        "        else:\n",
        "            print(f\"Unexpected response: {response}\")\n",
        "            return 'Error'\n",
        "\n",
        "    def _save_responses(self, responses, file_path):\n",
        "        \"\"\"\n",
        "        Save the responses DataFrame to a CSV file.\n",
        "        \"\"\"\n",
        "        folder_path = os.path.dirname(file_path)\n",
        "        if not os.path.exists(folder_path):\n",
        "            os.makedirs(folder_path)\n",
        "        responses.to_csv(file_path, index=False)\n",
        "        print(f'Responses saved successfully to: {file_path}')\n",
        "\n",
        "    def sample_questions_and_ask_claude(self, data, hypotheses, file_path):\n",
        "        \"\"\"\n",
        "        Sample questions, ask Claude for evaluation, and save the responses.\n",
        "        \"\"\"\n",
        "        responses = []\n",
        "\n",
        "        for index, hypothesis_row in hypotheses.iterrows():\n",
        "            failure_mode = hypothesis_row['Failure Mode Title']\n",
        "            description = hypothesis_row['Failure Mode Description']\n",
        "            print('failure mode:', failure_mode)\n",
        "\n",
        "            for question in data['question'].sample(2):\n",
        "                prompt_text = self._generate_prompt(failure_mode, description, question)\n",
        "                response = self.ask_claude(prompt_text)\n",
        "                print('model response',response)\n",
        "                is_falsifiable = self._interpret_response(response)\n",
        "                print('Response:', is_falsifiable)\n",
        "\n",
        "                responses.append({\n",
        "                    'Hypothesis': failure_mode,\n",
        "                    'Hypoth Descript': description,\n",
        "                    'Question': question,\n",
        "                    'Response': response,\n",
        "                    'Alignment Status': is_falsifiable\n",
        "                })\n",
        "\n",
        "        self._save_responses(pd.DataFrame(responses), file_path)\n",
        "        return pd.DataFrame(responses)\n",
        "\n",
        "    def _generate_prompt(self, failure_mode, description, question):\n",
        "        \"\"\"\n",
        "        Generate the prompt text for Claude.\n",
        "        \"\"\"\n",
        "        return f\"\"\"\n",
        "        Before answering, provide your reasoning.\n",
        "\n",
        "        Evaluate the alignment between the provided question and the hypothesis based on the title and description.\n",
        "        Importantly, a question should be considered aligned even if the aspect connecting it to the criteria is not\n",
        "        the primary focus of the question.\n",
        "\n",
        "        The question isn't required to completely satisfy all the criteria. If any part of the question, no matter how straightforward or simple,\n",
        "        meets any aspect of the criteria, it should be considered aligned.\n",
        "\n",
        "        Additionally, assess if the hypothesis description is too vague, ambiguous, or broadly interpreted to make\n",
        "        a confident judgment on alignment and explicitly state the reasons for this uncertainty.\n",
        "\n",
        "        Hypothesized Failure Mode: \"{failure_mode}\"\n",
        "        Description of Failure Mode: \"{description}\"\n",
        "        Question: \"{question}\"\n",
        "\n",
        "        1. Analysis: Examine the failure mode and its description along with the provided question. Identify any elements in the question that relate to the hypothesis, regardless of their prominence.\n",
        "\n",
        "        2. Reasoning: Discuss the connections or discrepancies found during the analysis. Explain why these points lead you to consider the question as aligned, unaligned, or too ambiguous to determine.\n",
        "\n",
        "        3. Conclusion: Provide your final assessment based on the analysis, reasoning, and hypothesis feedback.\n",
        "        - If the question aligns with the hypothesis, state 'Conclusion: Aligned'.\n",
        "        - If the question does not align with the hypothesis, state 'Conclusion: Unaligned'.\n",
        "        - If it's unclear whether the question aligns due to vagueness or ambiguity in the hypothesis, state 'Conclusion: Uncertain'. If the hypothesis is deemed too vague or ambiguous, provide specific reasons and suggest how it might be refined.\n",
        "        \"\"\"\n",
        "\n",
        "    def ask_claude(self, prompt_text):\n",
        "        \"\"\"\n",
        "        Send a prompt to the Claude Chat API and return the response.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            response = self.client.messages.create(\n",
        "                model=\"claude-3-opus-20240229\",\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": prompt_text}\n",
        "                ],\n",
        "                temperature=1,\n",
        "                max_tokens=1000,\n",
        "                top_p=1,\n",
        "            )\n",
        "            if response.content and len(response.content) > 0:\n",
        "                content_text = response.content[0].text.strip()\n",
        "                return content_text\n",
        "            else:\n",
        "                print(\"No content found in response.\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path_1 = 'critiqued_hypoth.csv'"
      ],
      "metadata": {
        "id": "UIx-cvkwOBqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add more threads for faster processing"
      ],
      "metadata": {
        "id": "VtsA0tY7O_pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cq_processor1 = ClaudeQueryProcessor(client)"
      ],
      "metadata": {
        "id": "3fA34jn2OJC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1=cq_processor1.sample_questions_and_ask_claude(data, failures_df[:1], file_path)"
      ],
      "metadata": {
        "id": "3iPdP28qP1VH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_responses=df1"
      ],
      "metadata": {
        "id": "dkAz-LvWOj4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_data = []\n",
        "\n",
        "for hypothesis in sampled_responses['Hypothesis'].unique():\n",
        "    temp_df = sampled_responses[sampled_responses['Hypothesis'] == hypothesis]\n",
        "    uncertain_count = (temp_df['Alignment Status'] == 'Uncertain').sum()\n",
        "    if uncertain_count <= 2:\n",
        "        filtered_data.append(temp_df)\n",
        "\n",
        "filtered_df = pd.concat(filtered_data, ignore_index=True)\n"
      ],
      "metadata": {
        "id": "3SO_tjmHPKpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_hypotheses_2 = filtered_df['Hypothesis'].unique()\n",
        "\n",
        "for hypothesis in unique_hypotheses_2:\n",
        "    print(f\"Hypothesis: {hypothesis}\")\n",
        "    filtered_df_2 = filtered_df[filtered_df['Hypothesis'] == hypothesis]\n",
        "    alignment_status_counts = filtered_df_2['Alignment Status'].value_counts()\n",
        "    print(alignment_status_counts)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "kNAoRUDFPOPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copied_df = filtered_df[['Hypothesis', 'Hypoth Descript']].copy().rename(\n",
        "    columns={'Hypothesis': 'Failure Mode Title', 'Hypoth Descript': 'Failure Mode Description'}\n",
        ")\n"
      ],
      "metadata": {
        "id": "emGUPZOaPVh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copied_df_no_duplicates = copied_df.drop_duplicates()\n",
        "critiqued_failures = copied_df_no_duplicates.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "wXqvQDidPYiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deduplication**"
      ],
      "metadata": {
        "id": "h-3NiQs2QYCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_url = \"\""
      ],
      "metadata": {
        "id": "6RvWNQ6_PZx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_description(description):\n",
        "    \"\"\"\n",
        "    Removes the leading numbering and any special formatting from the description.\n",
        "\n",
        "    :param description: A string representing the description.\n",
        "    :return: A cleaned string of the description.\n",
        "    \"\"\"\n",
        "    cleaned = re.sub(r'^\\d+\\.\\s*\\**', '', description).strip()\n",
        "    return cleaned\n",
        "\n",
        "def embed_descriptions(df, column_name, api_key, api_url):\n",
        "    \"\"\"\n",
        "    Embeds each description in the dataframe column using OpenAI API.\n",
        "\n",
        "    :param df: The dataframe containing the descriptions.\n",
        "    :param column_name: The name of the column with descriptions to be embedded.\n",
        "    :param api_key: OpenAI API key.\n",
        "    :param api_url: OpenAI API URL for embeddings.\n",
        "    :return: The dataframe with added column for embeddings.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    embeddings = []\n",
        "\n",
        "    for description in df[column_name]:\n",
        "        cleaned_description = clean_description(description)\n",
        "\n",
        "        data = {\n",
        "            \"model\": \"text-embedding-ada-002\",\n",
        "            \"input\": cleaned_description\n",
        "        }\n",
        "\n",
        "        response = requests.post(api_url, headers=headers, json=data)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            embedding = response.json()['data']\n",
        "            actual_embedding = embedding[0]['embedding']\n",
        "\n",
        "            embeddings.append(actual_embedding)\n",
        "        else:\n",
        "            print(f\"Error embedding description: {response.status_code} - {response.text}\")\n",
        "            embeddings.append(None)\n",
        "\n",
        "    df['embedding'] = embeddings\n",
        "    return df"
      ],
      "metadata": {
        "id": "-TZnycZTQXNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = embed_descriptions(critiqued_failures, 'Failure Mode Title', openai.api_key, api_url)\n"
      ],
      "metadata": {
        "id": "j02VI3cfQeFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_similar_rows(df, threshold=0.9):\n",
        "    \"\"\"\n",
        "    Removes rows from the dataframe with cosine similarity higher than the given threshold.\n",
        "\n",
        "    :param df: Dataframe with a column 'embedding' containing the embeddings.\n",
        "    :param threshold: The cosine similarity threshold to remove similar rows.\n",
        "    :return: A dataframe with highly similar rows removed.\n",
        "    \"\"\"\n",
        "    embeddings_matrix = np.vstack(df['embedding'])\n",
        "    similarity_matrix = cosine_similarity(embeddings_matrix)\n",
        "\n",
        "    rows_to_drop = set()\n",
        "    high_similarity_pairs = []\n",
        "\n",
        "    for i in range(len(similarity_matrix)):\n",
        "        for j in range(i+1, len(similarity_matrix)):\n",
        "            if similarity_matrix[i, j] > threshold:\n",
        "                rows_to_drop.add(j)\n",
        "                high_similarity_pairs.append((df.iloc[i]['Failure Mode Title'], df.iloc[j]['Failure Mode Title'], similarity_matrix[i, j]))\n",
        "\n",
        "    print(\"Failure Modes for rows to be dropped:\\n\")\n",
        "    for index in rows_to_drop:\n",
        "        print('Index:', index)\n",
        "        print(df.iloc[index]['Failure Mode Title'])\n",
        "\n",
        "    print(\"\\nPairs of questions with high similarity scores:\")\n",
        "    for pair in high_similarity_pairs:\n",
        "        print(f\"Q1: {pair[0]}\\nQ2: {pair[1]}\\nSimilarity Score: {pair[2]}\\n\")\n",
        "\n",
        "    # Drop identified rows\n",
        "    df_filtered = df.drop(index=list(rows_to_drop))\n",
        "    return df_filtered"
      ],
      "metadata": {
        "id": "blNgEhzAQfvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_deduplicated = remove_similar_rows(df)\n"
      ],
      "metadata": {
        "id": "GYGucU2dQqkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Validator**"
      ],
      "metadata": {
        "id": "gRleyMc0UYi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HypothesisVerifier:\n",
        "    def __init__(self, hypotheses, save_dir):\n",
        "        self.hypotheses = hypotheses\n",
        "        self.save_dir = save_dir\n",
        "        self._prepare_save_dir()\n",
        "\n",
        "    def ask_claude(self, prompt_text):\n",
        "        \"\"\"Send a prompt to the Claude Chat API and return the response.\"\"\"\n",
        "        try:\n",
        "            response = client.messages.create(\n",
        "                model=\"claude-3-opus-20240229\",\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": prompt_text}\n",
        "                ],\n",
        "                temperature=1,\n",
        "                max_tokens=1000,\n",
        "                top_p=1,\n",
        "\n",
        "            )\n",
        "            if response.content and len(response.content) > 0:\n",
        "                content_text = response.content[0].text.strip()\n",
        "                return content_text\n",
        "            else:\n",
        "                print(\"No content found in response.\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _prepare_save_dir(self):\n",
        "        if not os.path.exists(self.save_dir):\n",
        "            os.makedirs(self.save_dir)\n",
        "\n",
        "\n",
        "    def verify(self, hypothesis_index, matched_data, start_index):\n",
        "        hypothesis, description = self._get_hypothesis_info(hypothesis_index)\n",
        "        verifications = ['Pending'] * len(matched_data.loc[start_index:])\n",
        "        full_responses = [''] * len(matched_data.loc[start_index:])\n",
        "        processed_counter = start_index\n",
        "\n",
        "        for position, (index, row) in enumerate(matched_data.loc[start_index:].iterrows()):\n",
        "            processed_counter += 1\n",
        "            response = self._query_openai(self._construct_prompt(row['question'], hypothesis, description))\n",
        "            print('response', response)\n",
        "            if response:\n",
        "                verifications[position] = self._interpret_response(response)\n",
        "                full_responses[position] = response\n",
        "            else:\n",
        "                print(f\"API request failed for question {index}. Moving to the next question.\")\n",
        "                verifications[position] = 'Error'\n",
        "                full_responses[position] = 'API request failed'\n",
        "\n",
        "            self._handle_verification_update(matched_data, verifications, full_responses, start_index, processed_counter, hypothesis_index)\n",
        "\n",
        "        return matched_data\n",
        "\n",
        "    def _get_hypothesis_info(self, hypothesis_index):\n",
        "        hypothesis_row = self.hypotheses.iloc[hypothesis_index]\n",
        "        return hypothesis_row['Failure Mode Title'], hypothesis_row['Failure Mode Description']\n",
        "\n",
        "    def _query_openai(self, prompt_text):\n",
        "        try:\n",
        "            response = self.ask_claude(prompt_text)\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _construct_prompt(self, question_content, hypothesis, description):\n",
        "        prompt = (\n",
        "        f\"Before answering, provide your reasoning.\"\n",
        "        f\"Your job is to determine if the question is asking for information described in the task.\\n\\n\"\n",
        "        f\"Task Mode Title: \\\"{hypothesis}\\\"\\n\"\n",
        "        f\"Description of Task: \\\"{description}\\\"\\n\"\n",
        "        f\"Question: \\\"{question_content}\\\"\\n\\n\"\n",
        "        f\"Instructions:\\n\"\n",
        "        f\"1. Determine if the question is asking for information or assistance related to the task.\\n\"\n",
        "        f\"2. Carefully read the question and identify any elements or phrases in the question that are relevant to\"\n",
        "        f\"the task and its description.\\n\\n\"\n",
        "        f\"3. Conclusion: State 'Conclusion: Yes' if your reasoning identifies a connection, \"\n",
        "        f\"between the characteristics described in the task and the characteristics of the question.\"\n",
        "        f\"If, on the other hand, your analysis reveals no such relation, then state 'Conclusion: No'.\"\n",
        "        )\n",
        "        return prompt\n",
        "\n",
        "    def _interpret_response(self, response):\n",
        "        if 'Conclusion: Yes' in response:\n",
        "            return 'Yes'\n",
        "        elif 'Conclusion: No' in response:\n",
        "            return 'No'\n",
        "        else:\n",
        "            print(f\"Unexpected response: {response}\")\n",
        "            return 'Error'\n",
        "\n",
        "\n",
        "    def _handle_verification_update(self, matched_data, verifications, full_responses, start_index, processed_counter, hypothesis_index):\n",
        "        expected_length = len(matched_data.loc[start_index:])\n",
        "        if len(verifications) != expected_length:\n",
        "            self._adjust_verifications_length(verifications, expected_length)\n",
        "            self._adjust_verifications_length(full_responses, expected_length)\n",
        "\n",
        "        matched_data.loc[start_index:, 'verification'] = verifications\n",
        "        matched_data.loc[start_index:, 'Full Response'] = full_responses\n",
        "        if processed_counter % 100 == 0 or processed_counter == expected_length:\n",
        "            self._save_results_to_file(matched_data, hypothesis_index, processed_counter)\n",
        "\n",
        "    def _adjust_verifications_length(self, verifications, expected_length):\n",
        "        if len(verifications) > expected_length:\n",
        "            return verifications[:expected_length]\n",
        "        else:\n",
        "            verifications.extend(['Error'] * (expected_length - len(verifications)))\n",
        "\n",
        "    def _save_results_to_file(self, matched_data, hypothesis_index, processed_counter):\n",
        "        filename = f'hypothesis_results_{hypothesis_index}_{processed_counter}.csv'\n",
        "        matched_data.to_csv(os.path.join(self.save_dir, filename))\n",
        "        print(f'Results saved to {filename}')"
      ],
      "metadata": {
        "id": "Ux4mDfK3QvB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_database=pd.read_csv('/content/AROMA/human_feedback_category_scorer_data.csv')"
      ],
      "metadata": {
        "id": "RkwMIXLTRrgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "verifier1 = HypothesisVerifier(df_deduplicated, save_dir='/content/')\n",
        "\n",
        "final_df=verifier1.verify(hypothesis_index=0, matched_data=retrieval_database, start_index=1317)"
      ],
      "metadata": {
        "id": "uUkY_5tLRVrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value_counts = final_df['is_correct'].value_counts()\n",
        "true_count = value_counts.get(True, 0)\n",
        "false_count = value_counts.get(False, 0)\n",
        "\n",
        "total_count = true_count + false_count\n",
        "score = true_count / total_count if total_count > 0 else 0\n",
        "\n",
        "print(f\"Score: {score:.2f}\")\n",
        "print(f\"True count: {true_count}\")\n",
        "print(f\"False count: {false_count}\")"
      ],
      "metadata": {
        "id": "4Rf4uet7SM4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "verified_data=final_df[final_df['verification']=='Yes'].copy()\n",
        "\n",
        "verified_data_value_counts = verified_data['is_correct'].value_counts()\n",
        "verified_data_true_count = verified_data_value_counts.get(True, 0)\n",
        "verified_data_false_count = verified_data_value_counts.get(False, 0)\n",
        "\n",
        "verified_data_total_count = verified_data_true_count + verified_data_false_count\n",
        "verified_data_score = verified_data_true_count / verified_data_total_count if verified_data_total_count > 0 else 0\n",
        "\n",
        "print(f\"Score: {verified_data_score:.2f}\")\n",
        "print(f\"True count: {verified_data_true_count}\")\n",
        "print(f\"False count: {verified_data_false_count}\")"
      ],
      "metadata": {
        "id": "V9eZ4HkvTUcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xrFVJwwfTlPP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}